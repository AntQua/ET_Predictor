{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331c183e-0d53-4ceb-9219-51208df76792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d07ed8-ab82-4eef-b0f9-2ef40bec0351",
   "metadata": {},
   "source": [
    "Step 1: Load and Inspect the Dataset\n",
    "Load the dataset and display the first few rows to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da19f3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "  duration (seconds) duration (hours/min)  \\\n",
       "0               2700           45 minutes   \n",
       "1               7200              1-2 hrs   \n",
       "2                 20           20 seconds   \n",
       "3                 20             1/2 hour   \n",
       "4                900           15 minutes   \n",
       "\n",
       "                                            comments date posted    latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.8830556   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005    29.38421   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008        53.2   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.9783333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.4180556   \n",
       "\n",
       "   longitude   \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/antqua/code/AntQua/ET_Predictor/raw_data/scrubbed.csv'\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0962cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(data.columns)\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe81b54-a4a0-4df6-9e1b-65cf428124d5",
   "metadata": {},
   "source": [
    "The dataset has several columns, including datetime, city, state, country, shape, duration (seconds), duration (hours/min), comments, date posted, latitude, and longitude. Our focus will be on the datetime and duration (seconds) columns for features, and latitude and longitude for the target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b87d2-84ba-4557-97ad-82fca08239a7",
   "metadata": {},
   "source": [
    "Step 2: Data Preparation\n",
    "\n",
    "Convert the datetime column to numerical format and split it into separate features\n",
    "We'll convert the datetime column to datetime objects and then extract the year, month, day, hour, and minute as separate features.\n",
    "\n",
    "Handle Missing Values\n",
    "We'll check for and handle any missing values in the relevant columns (datetime, duration (seconds), latitude, and longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc5205-44cc-48fb-9e6b-7827b8640df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from all column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Convert latitude and longitude to numeric, forcing invalid parsing to NaN\n",
    "data['latitude'] = pd.to_numeric(data['latitude'], errors='coerce')\n",
    "data['longitude'] = pd.to_numeric(data['longitude'], errors='coerce')\n",
    "\n",
    "# Convert duration (seconds) to numeric, forcing invalid parsing to NaN\n",
    "data['duration (seconds)'] = pd.to_numeric(data['duration (seconds)'], errors='coerce')\n",
    "\n",
    "# Convert datetime column to datetime object\n",
    "data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')\n",
    "\n",
    "# Drop rows where datetime conversion failed\n",
    "data = data.dropna(subset=['datetime'])\n",
    "\n",
    "# Drop rows with NaN \n",
    "data = data.dropna()\n",
    "\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "# # Verify the conversion\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd8bd7-0bfe-49a4-9961-b2ad6747cc68",
   "metadata": {},
   "source": [
    "Step 3: Feature Engineering\n",
    "\n",
    "We'll use the extracted datetime features and the duration (seconds) for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32839b0-62f9-4248-8e2c-6918614eb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_data = data[data['country'] == 'us']\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "# us_only_file_path = '/mnt/data/us_only_scrubbed.csv'\n",
    "# us_data.to_csv(us_only_file_path, index=False)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"US Only Scrubbed Dataset\", dataframe=us_data)\n",
    "#us_only_file_path\n",
    "\n",
    "us_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75ccf8-413c-48b2-9262-d4331c54e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an explicit copy of the DataFrame\n",
    "us_data = us_data.copy()\n",
    "\n",
    "# Extract year, month, day, hour, and minute from datetime\n",
    "us_data['year'] = us_data['datetime'].dt.year\n",
    "us_data['month'] = us_data['datetime'].dt.month\n",
    "us_data['day'] = us_data['datetime'].dt.day\n",
    "us_data['hour'] = us_data['datetime'].dt.hour\n",
    "us_data['minute'] = us_data['datetime'].dt.minute\n",
    "\n",
    "# Specify the columns to keep\n",
    "#columns_to_keep = ['datetime', 'duration (seconds)', 'latitude', 'longitude', 'year', 'month', 'day', 'hour', 'minute']\n",
    "\n",
    "# Select only the specified columns\n",
    "#us_data = us_data.loc[:, columns_to_keep]\n",
    "\n",
    "# Display the head, dtypes, and shape of the DataFrame\n",
    "us_data_dtypes = us_data.dtypes\n",
    "us_data_shape = us_data.shape\n",
    "\n",
    "us_data_dtypes, us_data_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264c3f9-0bd9-46e6-88d2-032a4a02dcdb",
   "metadata": {},
   "source": [
    "Step 4: Model Training\n",
    "\n",
    "We'll scale the features and train both LinearRegression and RandomForestRegressor models for predicting latitude and longitude separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c0c1c-6a66-4429-856f-8e989ec680ce",
   "metadata": {},
   "source": [
    "Keep just the enntries for the united states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2c770-1be9-4a57-8068-a07e734abbf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define features and target variables\n",
    "features = ['duration (seconds)', 'year', 'month', 'day', 'hour', 'minute']\n",
    "X = us_data[features]\n",
    "y_lat = us_data['latitude']\n",
    "y_long = us_data['longitude']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_lat_train, y_lat_test, y_long_train, y_long_test = train_test_split(X, y_lat, y_long, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest Regressor models\n",
    "rf_reg_lat = RandomForestRegressor(random_state=42)\n",
    "rf_reg_lat.fit(X_train_scaled, y_lat_train)\n",
    "\n",
    "rf_reg_long = RandomForestRegressor(random_state=42)\n",
    "rf_reg_long.fit(X_train_scaled, y_long_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7168a-6fa2-4119-9ac3-abb09de6d23f",
   "metadata": {},
   "source": [
    "Step 5: Prediction\n",
    "We'll use the trained models to predict the location for a future chosen date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9d369-1730-46f7-bb8e-2523b778a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the models\n",
    "y_lat_pred_rf = rf_reg_lat.predict(X_test_scaled)\n",
    "y_long_pred_rf = rf_reg_long.predict(X_test_scaled)\n",
    "\n",
    "mae_lat_rf = median_absolute_error(y_lat_test, y_lat_pred_rf)\n",
    "mae_long_rf = median_absolute_error(y_long_test, y_long_pred_rf)\n",
    "\n",
    "mae_results = {\n",
    "    'Random Forest Latitude MAE': mae_lat_rf,\n",
    "    'Random Forest Longitude MAE': mae_long_rf,\n",
    "}\n",
    "\n",
    "mae_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f092aa-698b-4fba-b59c-4363b69ec577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction for a future chosen date\n",
    "future_date = pd.to_datetime('2024-10-04 00:00:00')\n",
    "future_duration = 120  # example duration in seconds\n",
    "\n",
    "future_features = pd.DataFrame({\n",
    "    'duration (seconds)': [future_duration],\n",
    "    'year': [future_date.year],\n",
    "    'month': [future_date.month],\n",
    "    'day': [future_date.day],\n",
    "    'hour': [future_date.hour],\n",
    "    'minute': [future_date.minute]\n",
    "})\n",
    "\n",
    "future_features_scaled = scaler.transform(future_features)\n",
    "\n",
    "# Predict latitude and longitude using both models\n",
    "predicted_lat_rf = rf_reg_lat.predict(future_features_scaled)\n",
    "predicted_long_rf = rf_reg_long.predict(future_features_scaled)\n",
    "\n",
    "predicted_location_rf = (predicted_lat_rf[0], predicted_long_rf[0])\n",
    "\n",
    "print(f\"Predicted location (Random Forest): {predicted_location_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8065b2-d792-469f-8d9a-e63af5b57651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "predicted_location_rf = (predicted_lat_rf, predicted_long_rf)\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Create a Basemap instance with a larger scale\n",
    "m = Basemap(projection='merc', llcrnrlat=predicted_lat_rf-15, urcrnrlat=predicted_lat_rf+15,\n",
    "            llcrnrlon=predicted_long_rf-30, urcrnrlon=predicted_long_rf+30, resolution='i')\n",
    "\n",
    "# Draw coastlines and countries\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "\n",
    "# Convert latitude and longitude to map projection coordinates\n",
    "x, y = m(predicted_long_rf, predicted_lat_rf)\n",
    "\n",
    "# Plot the predicted location\n",
    "m.plot(x, y, 'bo', markersize=12)\n",
    "plt.text(x, y, '   <---------', fontsize=12, ha='left', va='center', color='red')\n",
    "\n",
    "# Add title\n",
    "plt.title('Predicted UFO Location on Map')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6915c1-5b80-430d-a447-27192752c60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
